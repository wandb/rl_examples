{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa35b47-f5e8-400e-ab15-8987bf1f5cf0",
   "metadata": {},
   "source": [
    "# Trace rollouts with Weave to debug faster and reduce regressions\n",
    "\n",
    "_Authored by: [Aakash Kumar Nain](...)_\n",
    "\n",
    "\n",
    "This notebook extends the [TRL GRPO tutorial](https://huggingface.co/learn/cookbook/fine_tuning_llm_grpo_trl). We add `wandb` for logging, `Weave` for tracing, and small hyperparameter tweaks to show RL sensitivity. <br>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/rl_examples/blob/main/trl_examples/numinamath_grpo.ipynb)\n",
    "\n",
    "\n",
    "### Why trace rollouts?\n",
    "Compared to plain SFT, tuning models with RL is notoriously hard. On the surface, it may look very similar, especially if you are using packages like `TRL` for doing SFT and RL for LLMs, but it comes with many nuances. For example:\n",
    "\n",
    "1. Is the reward trajectory a good enough signal to track the progress of a training run? How do you know if it is a normal run or a side effect of reward hacking?\n",
    "2. What is the best way to define a grader? Is a static grader enough for grading your rollouts?\n",
    "3. Is the model following the constraints? For example, say we want the model to always generate the reasoning tokens within `<think>...</think>` tags and the answer within `<answer>...</answer>` tags. As training progresses, how do we know if the model is following the right format? What if it follows one format correctly but ignores the other one completely? How do we debug that during the training run?\n",
    "4. There is a fair chance that the model can generate structurally correct responses, yet the semantics of those responses are completely wrong. How far into the training run do we go before we decide to stop and start over, maybe with some changes?\n",
    "5. If your trajectory is rewarding, does that mean your responses are improving? How do you compare the completions for the same samples at different training steps?\n",
    "\n",
    "### Weave: The value proposition for RL\n",
    "Though Weave can do a lot more, it provides an easy solution to the above questions. For example, tracing with Weave looks like shown below. Here you can see the raw response as well as the rendered responses. This gives you an idea of how well-formed the model completions are as training progresses. It makes it easy to identify what the model is missing or overemphasizing during the course of training. <br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wandb/rl_examples//main/assets/weave_rollouts.png\" width=\"70%\"><br><br>\n",
    "\n",
    "\n",
    "\n",
    "You can even compare rollouts, as shown below. A good example of comparing rollouts is that it is easy to detect whether the model is following the correct formats, is on the brink of mode collapse, or is on the path of reward hacking. <br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wandb/rl_examples//main/assets/weave_compare_rollouts.png\" width=\"70%\">\n",
    "\n",
    "\n",
    "Without further ado, let's take a look at the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2c35d-9ad1-4ff4-8c69-acfb1f878b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install  -U -q trl peft math_verify wandb weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8376ed1b-4a7c-42cf-84bb-06c9facd2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Do not forget to set your wandb API key\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\"\n",
    "if not os.environ.get(\"WANDB_API_KEY\"):\n",
    "    raise ValueError(\"WANDB_API_KEY not found!\")\n",
    "\n",
    "# optional: set the project name\n",
    "os.environ[\"WANDB_PROJECT\"] = \"trl_grpo_numinamath_example\"\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import GRPOConfig, GRPOTrainer,  WeaveCallback\n",
    "from math_verify import LatexExtractionConfig, parse, verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9569a6c-293c-4dce-ad2a-5553ae17814f",
   "metadata": {},
   "source": [
    "### 1. Dataset\n",
    "For the demonstration purpose, we will sample a fraction of the training and test splits from the dataset and use them duringin this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ddbb05-3f14-4334-abe1-e7e892706ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'AI-MO/NuminaMath-TIR'\n",
    "train_dataset, eval_dataset = load_dataset(dataset_id, split=['train[:5%]', 'test[:25%]'])\n",
    "print(f\"Number of samples in the training dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in the evaluation dataset: {len(eval_dataset)}\")\n",
    "print(f\"Column names: {train_dataset.column_names}\")\n",
    "print(\"\\n======== Sample data point ============\")\n",
    "sample = train_dataset.take(1)[0]\n",
    "print(f\"\\nProblem:\\n{sample[\"problem\"]}\")\n",
    "print(f\"\\nSolution:\\n{sample[\"solution\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28caba0-602b-4785-9a64-999d46b09145",
   "metadata": {},
   "source": [
    "### 1.2 System Prompt\n",
    "We need to give the model clear instructions for generating responses. For example, we must tell the model to add special tags for the thinking process and the final answer. Let’s lay out those details in the prompt. You can try modifying the prompt and provide better instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3986f6-22cf-4f42-a0bc-6510f977a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = r\"\"\"You are an expert math solver. Solve the given question step by step. Strictly follow the format given below:\n",
    "\n",
    "- Use <think>...</think> tags for thinking process. All the intermediate steps for solving the question goes between these tags.\n",
    "- Use <answer>...</answer> tags for the answer. Answer should be strictly one word.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b697127-74ae-43d0-bd1a-b38e9e8a2b03",
   "metadata": {},
   "source": [
    "### 1.3 Modify the dataset\n",
    "We need to prepend the system prompt to every problem in the dataset, and convert the dataset into conversation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0878e-14e1-4a65-bb15-9ae6fbe1e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conversation(example):\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset.map(make_conversation)\n",
    "eval_dataset = eval_dataset.map(make_conversation)\n",
    "\n",
    "# Optional: Remove unsued columns from the training dataset\n",
    "train_dataset = train_dataset.remove_columns(['messages', 'problem'])\n",
    "\n",
    "# Check the samples again\n",
    "print(\"Sample data point\")\n",
    "sample = train_dataset.take(1)[0]\n",
    "print(f\"\\nProblem:\\n{sample[\"prompt\"][0][\"content\"]}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"\\nSolution:\\n{sample[\"solution\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cf239-0b57-499b-b78f-3949afa2953c",
   "metadata": {},
   "source": [
    "### 2. Model\n",
    "\n",
    "The original notebook used `Qwen2-0.5B-Instruct`. We will use `Qwen2.5-3B-Instruct` instead, because models at or above 3B parameters are usually more capable and follow instructions better. If you are resource constrained, choose any other model from the list given as per your hardware specs.\n",
    "\n",
    "- `Qwen/Qwen2.5-1.5B-Instruct`\n",
    "- `Qwen/Qwen2.5-0.5B-Instruct`\n",
    "- `Qwen/Qwen2-1.5B-Instruct`\n",
    "- `Qwen/Qwen2-0.5B-Instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632184e-7c2a-42e1-b43e-3b0120a5b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592baf4-7b11-4a62-9d84-8a6891d1e871",
   "metadata": {},
   "source": [
    "### 2.1 Configuring LoRA\n",
    "\n",
    "Next, we will configure LoRA for model training, allowing us to efficiently train the model with a reduced number of parameters. We will increase the `rank` from `8`(as used in the original code) to `16`. We will not use dropout in this example. These are the hyperparameters that you can play with to get a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdead44-e75c-42a8-99e2-41932285318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(task_type=\"CAUSAL_LM\", r=16, lora_alpha=32)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a4c81-9eed-49f0-8b73-979fc61842f0",
   "metadata": {},
   "source": [
    "### 2.2 Reward Functions\n",
    "We need to grade the model’s completions, so we need a grading system. We can use simple static checkers or a reward model. Choose based on the resources you have. For now, we will use static checkers to grade two aspects:\n",
    "\n",
    "1. **Format correctness:** Does the model generate responses in the correct format? In our case, the model should place thinking tokens inside `<think>...</think>` and the final answer inside `<answer>...</answer>`.\n",
    "2. **Numerical correctness:** Does the model produce the right answer for a given problem? In this example, we will use the `math_verify` package to check correctness.\n",
    "\n",
    "Here we have used partial correctness reward in the format check. It is optional, and you can do a simple `0-1` credit assignment here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a384a03-38c9-4a62-bdd4-9414d1691851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex patterns\n",
    "PATTERN_FULL = re.compile(r\"^<think>\\n?.*?\\n?</think>\\n?.*?\\n?<answer>\\n?.*?\\n?</answer>$\", re.DOTALL)\n",
    "PATTERN_THINK = re.compile(r\"<think>\\n?(.*?)\\n?</think>\", re.DOTALL)\n",
    "PATTERN_ANSWER = re.compile(r\"<answer>\\n?(.*?)\\n?</answer>\", re.DOTALL)\n",
    "\n",
    "\n",
    "def has_valid_format(text):\n",
    "    \"\"\"Check if text follows the full <think>...</think><answer>...</answer> structure.\"\"\"\n",
    "    return bool(PATTERN_FULL.match(text))\n",
    "\n",
    "def extract_sections(text):\n",
    "    \"\"\"Extracts the content inside <think> and <answer> tags.\n",
    "    Returns a tuple (think_text, answer_text), or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    think_match = PATTERN_THINK.search(text)\n",
    "    answer_match = PATTERN_ANSWER.search(text)\n",
    "    think_text = think_match.group(1).strip() if think_match else None\n",
    "    answer_text = answer_match.group(1).strip() if answer_match else None\n",
    "    return think_text, answer_text\n",
    "\n",
    "\n",
    "def accuracy_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion is the same as the ground truth.\"\"\"\n",
    "    solutions = kwargs['solution']\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = [0.0] * len(completion_contents)\n",
    "\n",
    "    for i, (ground_truth, prediction) in enumerate(zip(solutions, completion_contents)):\n",
    "        try:\n",
    "            gold_parsed = parse(\n",
    "                ground_truth,\n",
    "                extraction_mode=\"first_match\",\n",
    "                extraction_config=[LatexExtractionConfig()],\n",
    "                parsing_timeout=10, # keeping it 10 seconds for now\n",
    "            )\n",
    "            answer_parsed = parse(\n",
    "                prediction,\n",
    "                extraction_mode=\"first_match\",\n",
    "                extraction_config=[LatexExtractionConfig()],\n",
    "                parsing_timeout=10,\n",
    "            )\n",
    "        except Exception as ex:\n",
    "            print(\"Something went wrong when parsing predictions: \", ex)\n",
    "            gold_parsed = []\n",
    "            answer_parsed = []\n",
    "            \n",
    "        if len(gold_parsed) != 0:\n",
    "            try:\n",
    "                rewards[i] = float(verify(answer_parsed, gold_parsed))\n",
    "            except Exception:\n",
    "                rewards[i] =  0.0\n",
    "        else:\n",
    "            rewards[i] = 1.0\n",
    "    return rewards\n",
    "\n",
    "\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that gives partial credit:\n",
    "      +0.15 if <think> and <answer> structure exists else -0.15\n",
    "      +0.10 if <think> and <answer> tags contain text else -0.10\n",
    "      +0.25 if <answer> section is not empty else -0.25\n",
    "    \"\"\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = [0.0] * len(completion_contents)\n",
    "\n",
    "    for i, content in enumerate(completion_contents):\n",
    "        score = 0.0\n",
    "\n",
    "        if has_valid_format(content):\n",
    "            score += 0.15\n",
    "            think_text, answer_text = extract_sections(content)\n",
    "            # Another way to constrain the length of the thinking\n",
    "            # response. This is optional though, and you can modify\n",
    "            # it as you see fir for your use case\n",
    "            if 30 <= len(think_text.split()) <= 128:\n",
    "                score += 0.10\n",
    "            else:\n",
    "                score += -0.10\n",
    "            # We want the final answer to be strictly one-word.\n",
    "            # Another option could have been to remove this constraint but\n",
    "            # ask the model to put the final answer in \\boxed{}.\n",
    "            if len(answer_text.split()) == 1:\n",
    "                score += 0.25\n",
    "            else:\n",
    "                score += -0.25\n",
    "        else:\n",
    "            score += -0.15\n",
    "        rewards[i] = round(score, 2)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d83ea7-4e5e-4ffa-807f-a2b39aee5db9",
   "metadata": {},
   "source": [
    "### 2.3 Configuring GRPO Training Parameters\n",
    "\n",
    "Next, let us configure the training parameters for `GRPO`. The values here are slightly different from the original. It is recommended to try different hyperparameters and see how they affect the training run.\n",
    "\n",
    "PS: For starters, focus on `completion_length`, `num_generations`. For example:\n",
    "- Do you want your model to generate longer thoughts or keep them short?\n",
    "- What is the average number of reasoning tokens in the training set?\n",
    "- Should your model produce a chain of thought of similar length to the dataset or not?\n",
    "- Should you penalize the model if it starts drifting a lot?\n",
    "\n",
    "\n",
    "Also, hyperparameters such as `batch_size_per_device` and `gradient_accumulation_steps` can be set based on your hardware specs. These values are good enough to start, but there is plenty of room to tune them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf89dc-8ee6-49d3-98d0-10c1889f19a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-5\n",
    "lr_scheduler_type = \"linear\"\n",
    "weight_decay = 0.0\n",
    "remove_unused_columns = False # to access the solution column in accuracy_reward\n",
    "gradient_accumulation_steps = 4\n",
    "num_train_epochs = 1\n",
    "bf16 = True\n",
    "per_device_train_batch_size = 64\n",
    "gradient_checkpointing = True\n",
    "torch_compile = False\n",
    "\n",
    "num_generations = 4\n",
    "max_prompt_length = 256\n",
    "max_completion_length = 512\n",
    "temperature = 1.0\n",
    "top_p = 1.0\n",
    "top_k = None # turn off topk\n",
    "\n",
    "# Parameters related to reporting and saving\n",
    "report_to = [\"wandb\"]\n",
    "logging_steps = 5\n",
    "eval_steps = 5\n",
    "save_steps = 10\n",
    "push_to_hub = False\n",
    "save_strategy = \"steps\"\n",
    "eval_strategy = \"steps\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b7afc-4e17-4afa-a45e-b83ecd34af4e",
   "metadata": {},
   "source": [
    "### 2.4 Training\n",
    "Let us train the model now with the chosen hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b336e44-442f-4c84-9573-744df2d00caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments using GRPOConfig\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"qwen_numinamath_rl_3b\",\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    weight_decay=weight_decay,\n",
    "    remove_unused_columns=remove_unused_columns,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    bf16=bf16,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    torch_compile=torch_compile,\n",
    "\n",
    "    num_generations=num_generations,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_completion_length,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "\n",
    "    report_to=report_to,\n",
    "    logging_steps=logging_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    push_to_hub=push_to_hub,\n",
    "    save_strategy=save_strategy,\n",
    "    save_steps=save_steps,\n",
    "    eval_strategy=eval_strategy,\n",
    "\n",
    "    # Weights for the reward functions.\n",
    "    # You are free to change this and see observe the model behavior\n",
    "    reward_weights=[0.5, 0.5],\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[format_reward, accuracy_reward],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "# To trace the rollouts during training, we will use `weave`. TRL\n",
    "# already comes with a tight integration with weave. We can use the\n",
    "# `WeaveCallback` by attaching it to the trainer, and trace the\n",
    "# rollouts.\n",
    "\n",
    "# Init the callback\n",
    "weave_callback = WeaveCallback(\n",
    "    trainer=trainer,\n",
    "    # you can configure these generation params\n",
    "    generation_config=GenerationConfig(temperature=temperature, max_new_tokens=max_completion_length),\n",
    ")\n",
    "# Add the callback to the trainer\n",
    "trainer.add_callback(weave_callback)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c09c11-9bf5-4ad0-8959-db77e2e17abd",
   "metadata": {},
   "source": [
    "You can view the traces for the logged steps in the dashboard as show below:\n",
    "\n",
    "<img src=https://raw.githubusercontent.com/wandb/rl_examples//main/assets/trl/numinamath/grpo/1.png width=\"70%\">\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "You can then select any trace and view the logged prompts and completion as shown below:\n",
    "\n",
    "<img src=https://raw.githubusercontent.com/wandb/rl_examples//main/assets/trl/numinamath/grpo/2.png width=\"70%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982bcb7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hfenv)",
   "language": "python",
   "name": "hfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
